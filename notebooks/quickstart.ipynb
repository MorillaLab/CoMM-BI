# %% [markdown]
# # CoMM-BIP Quickstart Guide
# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/CoMM-BIP/blob/main/notebooks/quickstart.ipynb)
# 
# This notebook demonstrates:
# 1. Loading pretrained CoMM-BIP model
# 2. Running predictions on sample data
# 3. Visualising attention maps
# 4. Training from scratch (optional)

# %% [markdown]
# ## 1. Installation
# First, install required packages:

# %%
!pip install torch pytorch-lightning pandas scikit-learn matplotlib seaborn

# Clone repository (if not running on Colab)
import os
if not os.path.exists('CoMM-BIP'):
    !git clone https://github.com/yourusername/CoMM-BIP.git
    %cd CoMM-BIP

# %% [markdown]
# ## 2. Load Pretrained Model
# Download model weights from Zenodo:

# %%
import torch
from src.model import CoMM_BIP

MODEL_URL = "https://zenodo.org/record/16281076/files/CoMM-BIP_model.pt?download=1"

def load_pretrained_model():
    if not os.path.exists("models/CoMM-BIP_model.pt"):
        os.makedirs("models", exist_ok=True)
        !wget {MODEL_URL} -O models/CoMM-BIP_model.pt
    
    # Initialize model with dummy dimensions (will be overwritten)
    model = CoMM_BIP(
        input_dims={'rna': 1000, 'metabo': 50, 'pheno': 3, 'env': 1},
        feature_names={'rna': [], 'metabo': [], 'pheno': []},
        temperature=0.1
    )
    model.load_state_dict(torch.load("models/CoMM-BIP_model.pt"))
    model.eval()
    return model

model = load_pretrained_model()
print("✅ Model loaded successfully!")

# %% [markdown]
# ## 3. Prepare Sample Data
# Load and preprocess example data:

# %%
import pandas as pd
from src.data_loader import BiologicalDataLoader

# Initialize data loader
data_loader = BiologicalDataLoader(data_dir="data/sample_data")

# Get dataloaders
dataloaders = data_loader.get_dataloaders(augment_train=False)

# Inspect feature names
print("\nFeature counts:")
for modality, features in data_loader.feature_names.items():
    print(f"{modality}: {len(features)} features")

# Get one test batch
test_batch = next(iter(dataloaders['test']))
rna, metabo, pheno, env, labels = test_batch

# %% [markdown]
# ## 4. Run Predictions
# Make predictions on test data:

# %%
import numpy as np

# Predict effector classes
with torch.no_grad():
    logits = model(rna, metabo, pheno, env)
    probs = torch.softmax(logits, dim=1)
    preds = torch.argmax(probs, dim=1)

# Map to effector names
EFFECTOR_NAMES = {0: 'Control', 1: 'RiSP749', 2: 'GLOIN707', 3: 'GLOIN781'}

print("\nPredictions for first 5 samples:")
for i in range(5):
    print(f"Sample {i+1}:")
    print(f"  True: {EFFECTOR_NAMES[labels[i].item()]}")
    print(f"  Pred: {EFFECTOR_NAMES[preds[i].item()]} (confidence: {probs[i].max().item():.2f})")

# %% [markdown]
# ## 5. Visualize Attention
# Plot cross-modal attention weights:

# %%
import matplotlib.pyplot as plt
import seaborn as sns

def plot_attention(model, rna_features, metabo_features):
    # Get attention weights
    attn = model.rna_metabo_attn
    with torch.no_grad():
        Q = attn.query(rna.unsqueeze(1))
        K = attn.key(metabo.unsqueeze(1))
        attn_weights = torch.softmax(torch.bmm(Q, K.transpose(1, 2)) * torch.sigmoid(attn.bio_scale), dim=-1)
    
    # Plot
    plt.figure(figsize=(12, 6))
    sns.heatmap(attn_weights[0].numpy(), 
                xticklabels=metabo_features[:10],
                yticklabels=rna_features[:20],
                cmap="viridis")
    plt.title("Transcriptome-Metabolome Attention")
    plt.xlabel("Metabolites")
    plt.ylabel("Genes")
    plt.show()

# Plot for first sample
plot_attention(model, 
              data_loader.feature_names['rna'][:20], 
              data_loader.feature_names['metabo'][:10])

# %% [markdown]
# ## 6. Train from Scratch (Optional)
# Full training pipeline:

# %%
from src.train import train_model

if False:  # Set to True to run training
    trained_model, datamodule = train_model()
    
    # Save model
    torch.save(trained_model.state_dict(), "models/CoMM-BIP_trained.pt")
    print("Model saved!")

# %% [markdown]
# ## 7. Save Predictions
# Export results to CSV:

# %%
def save_predictions(dataloader, model, output_path="results/predictions.csv"):
    all_preds, all_probs, all_labels = [], [], []
    
    with torch.no_grad():
        for batch in dataloader:
            rna, metabo, pheno, env, labels = batch
            logits = model(rna, metabo, pheno, env)
            probs = torch.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)
            
            all_preds.extend(preds.numpy())
            all_probs.extend(probs.numpy())
            all_labels.extend(labels.numpy())
    
    results = pd.DataFrame({
        'True_Label': all_labels,
        'Predicted': all_preds,
        **{f'Prob_{name}': [p[i] for p in all_probs] 
           for i, name in EFFECTOR_NAMES.items()}
    })
    
    os.makedirs("results", exist_ok=True)
    results.to_csv(output_path, index=False)
    print(f"✅ Predictions saved to {output_path}")

save_predictions(dataloaders['test'], model)

# %% [markdown]
# ## Next Steps
# - Explore the `interpretability.ipynb` for SHAP analysis
# - Modify `config.yaml` for custom training
# - Visit our [documentation](docs/API.md) for advanced usage
